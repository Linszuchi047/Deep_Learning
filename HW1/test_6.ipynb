{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import transforms, datasets, models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465] \n",
    "std = [0.2470, 0.2435, 0.2616] \n",
    "batch_size = 40\n",
    "n_epochs = 100\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "    # Random augmentations\n",
    "    # Randomly rotate images by 40 degrees\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),  # Random color jitter\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalize with mean and std\n",
    "])\n",
    "\n",
    "path='train'\n",
    "all_train = datasets.ImageFolder(root = path, transform = train_transform)\n",
    "test = datasets.ImageFolder(root = 'ttest', transform = train_transform)\n",
    "train_size = int(0.9 * len(all_train))\n",
    "validation_size = len(all_train) - train_size\n",
    "train_dataset, validation_dataset = random_split(all_train , [train_size, validation_size])\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (3, 224, 224)  # Example: 3 channels, 32x32 pixels\n",
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import ceil\n",
    "\n",
    "# Inverted Residual Block with Squeeze-and-Excitation\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio, stride, kernel_size, reduction_ratio=4):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        \n",
    "        # Expansion phase\n",
    "        if expand_ratio != 1:\n",
    "            self.expand_conv = nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False)\n",
    "            self.bn0 = nn.BatchNorm2d(hidden_dim)\n",
    "        else:\n",
    "            self.expand_conv = None\n",
    "        \n",
    "        # Depthwise convolution\n",
    "        self.depthwise_conv = nn.Conv2d(hidden_dim if expand_ratio != 1 else in_channels, hidden_dim, \n",
    "                                        kernel_size=kernel_size, stride=stride, \n",
    "                                        padding=kernel_size // 2, groups=hidden_dim, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "        # Squeeze and Excitation block\n",
    "        self.se_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.se_fc1 = nn.Conv2d(hidden_dim, hidden_dim // reduction_ratio, kernel_size=1)\n",
    "        self.se_fc2 = nn.Conv2d(hidden_dim // reduction_ratio, hidden_dim, kernel_size=1)\n",
    "        \n",
    "        # Output phase\n",
    "        self.project_conv = nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.use_residual = (in_channels == out_channels and stride == 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        if self.expand_conv:\n",
    "            out = F.relu6(self.bn0(self.expand_conv(x)))\n",
    "        else:\n",
    "            out = x\n",
    "        \n",
    "        # Depthwise convolution\n",
    "        out = F.relu6(self.bn1(self.depthwise_conv(out)))\n",
    "        \n",
    "        # Squeeze and Excitation\n",
    "        se = self.se_avgpool(out)\n",
    "        se = F.relu(self.se_fc1(se))\n",
    "        se = torch.sigmoid(self.se_fc2(se))\n",
    "        out = out * se\n",
    "        \n",
    "        # Output\n",
    "        out = self.bn2(self.project_conv(out))\n",
    "        \n",
    "        if self.use_residual:\n",
    "            out = out + identity\n",
    "        \n",
    "        return out\n",
    "\n",
    "# EfficientNet Main Architecture\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, width_coefficient=0.6, depth_coefficient=0.7, dropout_rate=0.2, num_classes=100):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        \n",
    "        # Base settings for EfficientNet with reduced coefficients\n",
    "        base_channels = 16  # Further reduced base channels\n",
    "        base_layers = [\n",
    "            # (expand_ratio, out_channels, num_blocks, stride, kernel_size)\n",
    "            (1, 16, 1, 1, 3),   # Stage 1\n",
    "            (6, 24, 1, 2, 3),   # Stage 2\n",
    "            (6, 40, 1, 2, 5),   # Stage 3\n",
    "            (6, 80, 2, 2, 3),   # Stage 4\n",
    "            (6, 112, 2, 1, 5),  # Stage 5\n",
    "            (6, 192, 2, 2, 5),  # Stage 6\n",
    "            (6, 320, 1, 1, 3)   # Stage 7\n",
    "        ]\n",
    "        \n",
    "        # Stem\n",
    "        out_channels = ceil(base_channels * width_coefficient)\n",
    "        self.stem_conv = nn.Conv2d(3, out_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.stem_bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Build blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        in_channels = out_channels\n",
    "        for expand_ratio, out_channels, num_blocks, stride, kernel_size in base_layers:\n",
    "            out_channels = ceil(out_channels * width_coefficient)\n",
    "            num_blocks = ceil(num_blocks * depth_coefficient)\n",
    "            for i in range(num_blocks):\n",
    "                block_stride = stride if i == 0 else 1\n",
    "                self.blocks.append(MBConvBlock(in_channels, out_channels, expand_ratio, block_stride, kernel_size))\n",
    "                in_channels = out_channels\n",
    "        \n",
    "        # Head\n",
    "        final_channels = ceil(640 * width_coefficient)  # Further reduced head channels\n",
    "        self.head_conv = nn.Conv2d(in_channels, final_channels, kernel_size=1, bias=False)\n",
    "        self.head_bn = nn.BatchNorm2d(final_channels)\n",
    "        \n",
    "        # Pooling and classification\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(final_channels, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        x = F.relu6(self.stem_bn(self.stem_conv(x)))\n",
    "        \n",
    "        # Blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Head\n",
    "        x = F.relu6(self.head_bn(self.head_conv(x)))\n",
    "        \n",
    "        # Pooling and classification\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def efficientnet_custom(num_classes=100):\n",
    "    return EfficientNet(width_coefficient=0.6, depth_coefficient=0.7, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parameters: 1521845\n"
     ]
    }
   ],
   "source": [
    "model =EfficientNet().to(device)\n",
    "prune.l1_unstructured(model.fc, name='weight', amount=0.4)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"# parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [01:19<00:00,  3.94it/s]\n",
      "100%|██████████| 35/35 [00:17<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "Train Loss: 4.0365, Train Acc: 0.0667\n",
      "Val Loss: 3.7065, Val Acc: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:43<00:00,  7.32it/s]\n",
      "100%|██████████| 35/35 [00:11<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:\n",
      "Train Loss: 3.4492, Train Acc: 0.1374\n",
      "Val Loss: 3.4044, Val Acc: 0.1471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [1:25:49<00:00, 16.35s/it]     \n",
      "100%|██████████| 35/35 [00:13<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:\n",
      "Train Loss: 3.1525, Train Acc: 0.1928\n",
      "Val Loss: 3.1098, Val Acc: 0.1864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:30<00:00, 10.19it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100:\n",
      "Train Loss: 2.9243, Train Acc: 0.2385\n",
      "Val Loss: 2.8535, Val Acc: 0.2479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:30<00:00, 10.22it/s]\n",
      "100%|██████████| 35/35 [00:08<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100:\n",
      "Train Loss: 2.7502, Train Acc: 0.2765\n",
      "Val Loss: 2.7160, Val Acc: 0.2971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:30<00:00, 10.40it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100:\n",
      "Train Loss: 2.5713, Train Acc: 0.3157\n",
      "Val Loss: 2.6942, Val Acc: 0.3136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:29<00:00, 10.57it/s]\n",
      "100%|██████████| 35/35 [00:08<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100:\n",
      "Train Loss: 2.4379, Train Acc: 0.3439\n",
      "Val Loss: 2.3604, Val Acc: 0.3757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:30<00:00, 10.33it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100:\n",
      "Train Loss: 2.2940, Train Acc: 0.3823\n",
      "Val Loss: 2.2744, Val Acc: 0.3857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:30<00:00, 10.47it/s]\n",
      "100%|██████████| 35/35 [00:08<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100:\n",
      "Train Loss: 2.1815, Train Acc: 0.4039\n",
      "Val Loss: 2.2017, Val Acc: 0.4057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:31<00:00,  9.86it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100:\n",
      "Train Loss: 2.0673, Train Acc: 0.4272\n",
      "Val Loss: 2.1053, Val Acc: 0.4264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:37<00:00,  8.34it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100:\n",
      "Train Loss: 1.9566, Train Acc: 0.4620\n",
      "Val Loss: 1.9990, Val Acc: 0.4529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.27it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100:\n",
      "Train Loss: 1.8632, Train Acc: 0.4816\n",
      "Val Loss: 1.9560, Val Acc: 0.4771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.24it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100:\n",
      "Train Loss: 1.7676, Train Acc: 0.5086\n",
      "Val Loss: 1.8546, Val Acc: 0.4907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.36it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100:\n",
      "Train Loss: 1.7176, Train Acc: 0.5156\n",
      "Val Loss: 1.7892, Val Acc: 0.5207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.18it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100:\n",
      "Train Loss: 1.6209, Train Acc: 0.5451\n",
      "Val Loss: 1.7754, Val Acc: 0.5164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.36it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100:\n",
      "Train Loss: 1.5541, Train Acc: 0.5614\n",
      "Val Loss: 1.7054, Val Acc: 0.5243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.44it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100:\n",
      "Train Loss: 1.5023, Train Acc: 0.5761\n",
      "Val Loss: 1.6052, Val Acc: 0.5536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.43it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100:\n",
      "Train Loss: 1.4121, Train Acc: 0.5961\n",
      "Val Loss: 1.5834, Val Acc: 0.5664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.42it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100:\n",
      "Train Loss: 1.3773, Train Acc: 0.6101\n",
      "Val Loss: 1.5409, Val Acc: 0.5843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.35it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100:\n",
      "Train Loss: 1.3158, Train Acc: 0.6244\n",
      "Val Loss: 1.5432, Val Acc: 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.33it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100:\n",
      "Train Loss: 1.2685, Train Acc: 0.6354\n",
      "Val Loss: 1.5047, Val Acc: 0.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.28it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100:\n",
      "Train Loss: 1.2128, Train Acc: 0.6494\n",
      "Val Loss: 1.4966, Val Acc: 0.5893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.36it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100:\n",
      "Train Loss: 1.1652, Train Acc: 0.6617\n",
      "Val Loss: 1.4489, Val Acc: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.39it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100:\n",
      "Train Loss: 1.1300, Train Acc: 0.6708\n",
      "Val Loss: 1.4476, Val Acc: 0.6064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:40<00:00,  7.79it/s]\n",
      "100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100:\n",
      "Train Loss: 1.0916, Train Acc: 0.6810\n",
      "Val Loss: 1.4807, Val Acc: 0.5893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:55<00:00,  5.71it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100:\n",
      "Train Loss: 1.0384, Train Acc: 0.6970\n",
      "Val Loss: 1.3778, Val Acc: 0.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.51it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100:\n",
      "Train Loss: 0.9980, Train Acc: 0.7070\n",
      "Val Loss: 1.4101, Val Acc: 0.6221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.29it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100:\n",
      "Train Loss: 0.9674, Train Acc: 0.7169\n",
      "Val Loss: 1.3472, Val Acc: 0.6264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.44it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100:\n",
      "Train Loss: 0.9300, Train Acc: 0.7234\n",
      "Val Loss: 1.3068, Val Acc: 0.6314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.42it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100:\n",
      "Train Loss: 0.9117, Train Acc: 0.7302\n",
      "Val Loss: 1.3464, Val Acc: 0.6350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.45it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100:\n",
      "Train Loss: 0.8661, Train Acc: 0.7428\n",
      "Val Loss: 1.3378, Val Acc: 0.6443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.26it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100:\n",
      "Train Loss: 0.8525, Train Acc: 0.7454\n",
      "Val Loss: 1.3006, Val Acc: 0.6543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.35it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100:\n",
      "Train Loss: 0.8010, Train Acc: 0.7619\n",
      "Val Loss: 1.2450, Val Acc: 0.6579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.46it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100:\n",
      "Train Loss: 0.7732, Train Acc: 0.7703\n",
      "Val Loss: 1.2991, Val Acc: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.48it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100:\n",
      "Train Loss: 0.7480, Train Acc: 0.7732\n",
      "Val Loss: 1.2860, Val Acc: 0.6614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.41it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100:\n",
      "Train Loss: 0.7076, Train Acc: 0.7838\n",
      "Val Loss: 1.2316, Val Acc: 0.6714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.49it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100:\n",
      "Train Loss: 0.6948, Train Acc: 0.7888\n",
      "Val Loss: 1.2560, Val Acc: 0.6471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.45it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100:\n",
      "Train Loss: 0.6608, Train Acc: 0.7996\n",
      "Val Loss: 1.2184, Val Acc: 0.6657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.38it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100:\n",
      "Train Loss: 0.6358, Train Acc: 0.8088\n",
      "Val Loss: 1.1929, Val Acc: 0.6950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.36it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100:\n",
      "Train Loss: 0.6170, Train Acc: 0.8135\n",
      "Val Loss: 1.2400, Val Acc: 0.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.42it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100:\n",
      "Train Loss: 0.5864, Train Acc: 0.8218\n",
      "Val Loss: 1.2000, Val Acc: 0.6843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.47it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100:\n",
      "Train Loss: 0.5635, Train Acc: 0.8271\n",
      "Val Loss: 1.1861, Val Acc: 0.6936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.45it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100:\n",
      "Train Loss: 0.5393, Train Acc: 0.8351\n",
      "Val Loss: 1.1877, Val Acc: 0.6836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.52it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100:\n",
      "Train Loss: 0.5269, Train Acc: 0.8368\n",
      "Val Loss: 1.2383, Val Acc: 0.6786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.39it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100:\n",
      "Train Loss: 0.5000, Train Acc: 0.8429\n",
      "Val Loss: 1.1442, Val Acc: 0.6986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.26it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100:\n",
      "Train Loss: 0.4923, Train Acc: 0.8480\n",
      "Val Loss: 1.1762, Val Acc: 0.7171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.30it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100:\n",
      "Train Loss: 0.4594, Train Acc: 0.8563\n",
      "Val Loss: 1.1142, Val Acc: 0.7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.50it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100:\n",
      "Train Loss: 0.4309, Train Acc: 0.8677\n",
      "Val Loss: 1.1728, Val Acc: 0.6957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.41it/s]\n",
      "100%|██████████| 35/35 [00:11<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100:\n",
      "Train Loss: 0.4236, Train Acc: 0.8706\n",
      "Val Loss: 1.1681, Val Acc: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.29it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100:\n",
      "Train Loss: 0.4142, Train Acc: 0.8738\n",
      "Val Loss: 1.1676, Val Acc: 0.7121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.27it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100:\n",
      "Train Loss: 0.3863, Train Acc: 0.8786\n",
      "Val Loss: 1.1777, Val Acc: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.32it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100:\n",
      "Train Loss: 0.3852, Train Acc: 0.8808\n",
      "Val Loss: 1.1514, Val Acc: 0.7171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.23it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100:\n",
      "Train Loss: 0.3606, Train Acc: 0.8877\n",
      "Val Loss: 1.1308, Val Acc: 0.7229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.26it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100:\n",
      "Train Loss: 0.3392, Train Acc: 0.8960\n",
      "Val Loss: 1.1217, Val Acc: 0.7250\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=False)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=1e-6)\n",
    "\n",
    "# Early stopping class\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Train function\n",
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "# Validation function\n",
    "@torch.no_grad()\n",
    "def validate(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        val_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    avg_loss = val_loss / len(test_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training loop\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "early_stopper = EarlyStopper(patience=7)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, loss_fn)\n",
    "    val_loss, val_acc = validate(model, val_loader, loss_fn)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopper.early_stop(val_loss):\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), \"final_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.93%\n",
      "Test Accuracy: 0.9392857142857143\n",
      "Predictions saved to result.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Custom Dataset for Test Images\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# Import the custom EfficientNet model\n",
    "# prune.remove(model.fc, 'weight')\n",
    "\n",
    "# # Then save the model state\n",
    "# torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "def test():\n",
    "    # Load the trained model\n",
    "    model = EfficientNet()  # Initialize the model with 100 classes\n",
    "    model.load_state_dict(torch.load('best_model.pth'))  # Assuming you saved the trained model as model.pth\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # # Data transformations\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Resize((224, 224)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    # ])\n",
    "\n",
    "    # Load the test dataset\n",
    "    # test_dataset = datasets.ImageFolder(root='ttest', transform=transform)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "    print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_dir, transform=None):\n",
    "        self.test_dir = test_dir\n",
    "        self.file_names = os.listdir(test_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.file_names[idx]\n",
    "        img_path = os.path.join(self.test_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name\n",
    "\n",
    "# Load Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = efficientnet_custom(num_classes=100).to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Define transformations for train and test datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "    # Random augmentations\n",
    "    # Randomly rotate images by 40 degrees\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),  # Random color jitter\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalize with mean and std\n",
    "])\n",
    "\n",
    "# Load the train dataset using ImageFolder\n",
    "train_dir = os.path.abspath('./train')  # Replace with the path to your train dataset\n",
    "train_dataset = ImageFolder(train_dir, transform=transform)\n",
    "\n",
    "# Get the class_to_idx from the training dataset and invert it to create idx_to_class\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}  # Invert class_to_idx to idx_to_class\n",
    "\n",
    "# Load test dataset\n",
    "test_dir = os.path.abspath('./test')  # Path to your test dataset\n",
    "test_dataset = TestDataset(test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Prepare CSV for results\n",
    "results = []\n",
    "\n",
    "# Make predictions and store them in result.csv\n",
    "with torch.no_grad():\n",
    "    for images, file_names in test_loader:  # file_names returned from the dataset\n",
    "        images = images.to(device)  # Move images to GPU/CPU\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Get top-5 predictions for each image\n",
    "        _, top5_preds = torch.topk(outputs, 5, dim=1)\n",
    "        \n",
    "        for i, preds in enumerate(top5_preds):\n",
    "            file_name = file_names[i]  # Get the file name\n",
    "            # Convert predicted indices to class labels (words)\n",
    "            pred_labels = [idx_to_class[pred.item()] for pred in preds]\n",
    "            \n",
    "            # Append to results\n",
    "            results.append([file_name] + pred_labels)\n",
    "\n",
    "# Save results to CSV\n",
    "df = pd.DataFrame(results, columns=['file_name', 'pred1', 'pred2', 'pred3', 'pred4', 'pred5'])\n",
    "df.to_csv('result.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to result.csv\")\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# def test():\n",
    "#     # Load the trained model\n",
    "#     model = efficientnet_custom(num_classes=100) \n",
    "#     model.load_state_dict(torch.load('best_model.pth'))  # Load the trained model\n",
    "#     model.eval()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     # Data transformations\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     # Load the test dataset\n",
    "#     test_dataset = datasets.ImageFolder(root='ttest', transform=transform)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "#      # Mapping from index to class label (word)\n",
    "#     idx_to_class = {v: k for k, v in test_dataset.class_to_idx.items()}\n",
    "\n",
    "#     # Create CSV file with predictions\n",
    "#     with open('result.csv', 'w') as f:\n",
    "#         # Write header\n",
    "#         f.write('file_name,pred1,pred2,pred3,pred4,pred5\\n')\n",
    "\n",
    "#         # Make predictions and store them in result.csv\n",
    "#         with torch.no_grad():\n",
    "#             for i, (images, labels) in enumerate(test_loader):\n",
    "#                 outputs = model(images)\n",
    "#                 _, top5_preds = torch.topk(outputs, 5, dim=1)\n",
    "\n",
    "#                 for j, preds in enumerate(top5_preds):\n",
    "#                     # Get the file name for the current image\n",
    "#                     file_name = test_dataset.samples[i * test_loader.batch_size + j][0]\n",
    "\n",
    "#                     # Convert predicted indices to class labels (words)\n",
    "#                     pred_labels = [idx_to_class[pred.item()] for pred in preds]\n",
    "\n",
    "#                     # Write to CSV\n",
    "#                     f.write(f'{file_name},{pred_labels[0]},{pred_labels[1]},{pred_labels[2]},{pred_labels[3]},{pred_labels[4]}\\n')\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test()\n",
    "\n",
    "#     results = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images = images.to(device)\n",
    "#             outputs = model(images)\n",
    "#             # Get the top 5 predictions\n",
    "#             _, top5 = torch.topk(outputs, 5, dim=1)\n",
    "\n",
    "#             # Loop through the batch and save results\n",
    "#             for i, idx in enumerate(top5):\n",
    "#                 file_name = test_dataset.samples[i][0]  # Get the image file path\n",
    "#                 file_name = os.path.basename(file_name)  # Get the file name only\n",
    "#                 pred_labels = idx.cpu().numpy().tolist()  # Convert to list of top 5 predictions\n",
    "#                 results.append([file_name] + pred_labels)\n",
    "\n",
    "#     # Write the results to a CSV file\n",
    "#     with open('result.csv', mode='w', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(['file_name', 'pred1', 'pred2', 'pred3', 'pred4', 'pred5'])  # Write header\n",
    "#         writer.writerows(results)  # Write all rows of results\n",
    "\n",
    "#     print(f'Results saved to result.csv')\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import transforms, datasets, models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_model = models.mobilenet_v2()\n",
    "num_ftrs = bird_model.classifier[1].in_features\n",
    "bird_model.classifier[1]=nn.Linear(num_ftrs, 120,bias=True)\n",
    "bird_model.load_state_dict(torch.load('dog_model.pt'))\n",
    "\n",
    "for param in bird_model.parameters():\n",
    "    param.requires_grad = False\n",
    "# ct = 0\n",
    "# for child in bird_model.children():\n",
    "#     ct += 1\n",
    "#     if ct <= 20:\n",
    "#         for param in child.parameters():\n",
    "#             param.requires_grad = False\n",
    "    \n",
    "num_ftrs = bird_model.classifier[1].in_features\n",
    "bird_model.classifier[1]=nn.Linear(num_ftrs, 25,bias=True)\n",
    "\n",
    "for param in bird_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bird_model = bird_model.to(device)    \n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465] \n",
    "std = [0.2470, 0.2435, 0.2616] \n",
    "batch_size = 64\n",
    "n_epochs = 100\n",
    "\n",
    "train_transform = transforms.Compose([ \n",
    "transforms.Resize((224,224)), \n",
    "transforms.RandomCrop(224, padding=4), \n",
    "transforms.RandomHorizontalFlip(),\n",
    "transforms.ToTensor(), \n",
    "transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "path='C:/Users/User/DeepLearning/Deep_Learning/Bird_Classification/HW2_Dataset/dataset_new'\n",
    "all_train = datasets.ImageFolder(root = path, transform = train_transform)\n",
    "train_size = int(0.9 * len(all_train ))\n",
    "validation_size = len(all_train) - train_size\n",
    "train_dataset, validation_dataset = random_split(all_train , [train_size, validation_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bird_model.parameters(), lr=1e-3)\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    corrects=0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad() # step 1\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "\n",
    "        logits = model(images) # step 2 (forward pass)\n",
    "        loss = loss_fn(logits, labels) # step 3 (compute loss)\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        corrects += predictions.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        loss.backward() # step 4 (backpropagation)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "       \n",
    "        \n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    \n",
    "    return train_loss, corrects/total    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, valid_loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses=0.\n",
    "    corrects=0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "                \n",
    "            logits = model(images) # step 2 (forward pass)\n",
    "            loss = loss_fn(logits, labels) # step 3 (compute loss)\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            corrects += predictions.eq(labels).sum().item()\n",
    "            \n",
    "            losses += loss.item()*images.size(0)    \n",
    "            \n",
    "        valid_loss = losses/len(valid_loader.sampler)\n",
    "    return valid_loss, corrects / total\n",
    "\n",
    "\n",
    "# is_valid_available = True\n",
    "# scheduler = lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.9 ** epoch)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0)\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "early_stopper = EarlyStopper(patience=7)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    training_loss, training_accuracy = train(bird_model, train_loader, optimizer, loss_fn)\n",
    "    valid_loss, valid_accuracy = validate(bird_model, val_loader, loss_fn)\n",
    "    \n",
    "    train_loss_list.append(training_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "\n",
    "    # if scheduler is not None and is_valid_available:\n",
    "    #     scheduler.step(valid_loss)\n",
    "    # elif scheduler is not None:\n",
    "    #     scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}: training accuracy: {training_accuracy}, valid accuracy: {valid_accuracy}\")\n",
    "    \n",
    "    if early_stopper.early_stop(valid_loss): \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:16<00:00,  1.05it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: training accuracy: 0.10638297872340426, valid accuracy: 0.2066115702479339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.37it/s]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: training accuracy: 0.26549491211840887, valid accuracy: 0.2809917355371901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.43it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: training accuracy: 0.29324699352451433, valid accuracy: 0.256198347107438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.50it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: training accuracy: 0.3441258094357077, valid accuracy: 0.2975206611570248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.48it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: training accuracy: 0.34875115633672527, valid accuracy: 0.2809917355371901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.42it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: training accuracy: 0.3598519888991674, valid accuracy: 0.35537190082644626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.46it/s]\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: training accuracy: 0.3598519888991674, valid accuracy: 0.3305785123966942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.50it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: training accuracy: 0.38482886216466233, valid accuracy: 0.3305785123966942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.41it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: training accuracy: 0.364477335800185, valid accuracy: 0.30578512396694213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.51it/s]\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: training accuracy: 0.393154486586494, valid accuracy: 0.2644628099173554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.51it/s]\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: training accuracy: 0.38482886216466233, valid accuracy: 0.3140495867768595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.50it/s]\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: training accuracy: 0.39037927844588344, valid accuracy: 0.34710743801652894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: training accuracy: 0.4181313598519889, valid accuracy: 0.2892561983471074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.52it/s]\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: training accuracy: 0.40888066604995377, valid accuracy: 0.2727272727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.37it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: training accuracy: 0.4144310823311748, valid accuracy: 0.32231404958677684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.42it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: training accuracy: 0.42923219241443106, valid accuracy: 0.32231404958677684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.50it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: training accuracy: 0.42645698427382056, valid accuracy: 0.2892561983471074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.34it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: training accuracy: 0.42645698427382056, valid accuracy: 0.2892561983471074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.39it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: training accuracy: 0.4366327474560592, valid accuracy: 0.3140495867768595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.33it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: training accuracy: 0.4320074005550416, valid accuracy: 0.34710743801652894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.42it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: training accuracy: 0.45420906567992597, valid accuracy: 0.33884297520661155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.45it/s]\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: training accuracy: 0.44495837187789083, valid accuracy: 0.3140495867768595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.44it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: training accuracy: 0.4514338575393155, valid accuracy: 0.2975206611570248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.44it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: training accuracy: 0.45605920444033304, valid accuracy: 0.34710743801652894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.46it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: training accuracy: 0.45420906567992597, valid accuracy: 0.33884297520661155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: training accuracy: 0.4597594819611471, valid accuracy: 0.3305785123966942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.33it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: training accuracy: 0.46253469010175763, valid accuracy: 0.3140495867768595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:13<00:00,  1.27it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: training accuracy: 0.45790934320074006, valid accuracy: 0.2975206611570248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bird_model = models.mobilenet_v2()\n",
    "num_ftrs = bird_model.classifier[1].in_features\n",
    "bird_model.classifier[1]=nn.Linear(num_ftrs, 120,bias=True)\n",
    "bird_model.load_state_dict(torch.load('dog_model.pt'))\n",
    "\n",
    "# for param in bird_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "ct = 0\n",
    "for child in bird_model.children():\n",
    "    ct += 1\n",
    "    if ct <= 10:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "num_ftrs = bird_model.classifier[1].in_features\n",
    "bird_model.classifier[1]=nn.Linear(num_ftrs, 25,bias=True)\n",
    "\n",
    "for param in bird_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bird_model = bird_model.to(device)    \n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465] \n",
    "std = [0.2470, 0.2435, 0.2616] \n",
    "batch_size = 64\n",
    "n_epochs = 100\n",
    "\n",
    "train_transform = transforms.Compose([ \n",
    "transforms.Resize((224,224)), \n",
    "transforms.RandomCrop(224, padding=4), \n",
    "transforms.RandomHorizontalFlip(),\n",
    "transforms.ToTensor(), \n",
    "transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "path='C:/Users/User/DeepLearning/Deep_Learning/Bird_Classification/HW2_Dataset/dataset_new'\n",
    "all_train = datasets.ImageFolder(root = path, transform = train_transform)\n",
    "train_size = int(0.9 * len(all_train ))\n",
    "validation_size = len(all_train) - train_size\n",
    "train_dataset, validation_dataset = random_split(all_train , [train_size, validation_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bird_model.parameters(), lr=1e-3)\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    corrects=0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad() # step 1\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "\n",
    "        logits = model(images) # step 2 (forward pass)\n",
    "        loss = loss_fn(logits, labels) # step 3 (compute loss)\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        corrects += predictions.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        loss.backward() # step 4 (backpropagation)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "       \n",
    "        \n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    \n",
    "    return train_loss, corrects/total    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, valid_loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses=0.\n",
    "    corrects=0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "                \n",
    "            logits = model(images) # step 2 (forward pass)\n",
    "            loss = loss_fn(logits, labels) # step 3 (compute loss)\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            corrects += predictions.eq(labels).sum().item()\n",
    "            \n",
    "            losses += loss.item()*images.size(0)    \n",
    "            \n",
    "        valid_loss = losses/len(valid_loader.sampler)\n",
    "    return valid_loss, corrects / total\n",
    "\n",
    "\n",
    "# is_valid_available = True\n",
    "# scheduler = lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.9 ** epoch)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0)\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "early_stopper = EarlyStopper(patience=7)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    training_loss, training_accuracy = train(bird_model, train_loader, optimizer, loss_fn)\n",
    "    valid_loss, valid_accuracy = validate(bird_model, val_loader, loss_fn)\n",
    "    \n",
    "    train_loss_list.append(training_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "\n",
    "    # if scheduler is not None and is_valid_available:\n",
    "    #     scheduler.step(valid_loss)\n",
    "    # elif scheduler is not None:\n",
    "    #     scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}: training accuracy: {training_accuracy}, valid accuracy: {valid_accuracy}\")\n",
    "    \n",
    "    if early_stopper.early_stop(valid_loss): \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.45it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: training accuracy: 0.08695652173913043, valid accuracy: 0.1487603305785124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.33it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: training accuracy: 0.24699352451433856, valid accuracy: 0.23140495867768596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: training accuracy: 0.2969472710453284, valid accuracy: 0.24793388429752067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: training accuracy: 0.32932469935245146, valid accuracy: 0.2644628099173554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.32it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: training accuracy: 0.34782608695652173, valid accuracy: 0.2809917355371901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.46it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: training accuracy: 0.3617021276595745, valid accuracy: 0.2644628099173554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: training accuracy: 0.3598519888991674, valid accuracy: 0.32231404958677684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: training accuracy: 0.3700277520814061, valid accuracy: 0.3140495867768595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:12<00:00,  1.34it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: training accuracy: 0.3820536540240518, valid accuracy: 0.33884297520661155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.49it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: training accuracy: 0.40425531914893614, valid accuracy: 0.256198347107438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: training accuracy: 0.3691026827012026, valid accuracy: 0.33884297520661155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.46it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: training accuracy: 0.38390379278445885, valid accuracy: 0.3140495867768595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.48it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: training accuracy: 0.4005550416281221, valid accuracy: 0.3305785123966942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: training accuracy: 0.40795559666975023, valid accuracy: 0.3140495867768595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: training accuracy: 0.4421831637372803, valid accuracy: 0.3140495867768595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.48it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: training accuracy: 0.4144310823311748, valid accuracy: 0.3305785123966942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.50it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: training accuracy: 0.41720629047178537, valid accuracy: 0.3305785123966942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.44it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: training accuracy: 0.4181313598519889, valid accuracy: 0.34710743801652894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.42it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: training accuracy: 0.4320074005550416, valid accuracy: 0.35537190082644626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: training accuracy: 0.42923219241443106, valid accuracy: 0.30578512396694213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: training accuracy: 0.4301572617946346, valid accuracy: 0.35537190082644626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.49it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: training accuracy: 0.4440333024976873, valid accuracy: 0.32231404958677684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: training accuracy: 0.4357076780758557, valid accuracy: 0.2975206611570248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.48it/s]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: training accuracy: 0.4597594819611471, valid accuracy: 0.33884297520661155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.49it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: training accuracy: 0.47178538390379277, valid accuracy: 0.38016528925619836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.49it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: training accuracy: 0.4606845513413506, valid accuracy: 0.30578512396694213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.48it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: training accuracy: 0.4616096207215541, valid accuracy: 0.33884297520661155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.45it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: training accuracy: 0.47178538390379277, valid accuracy: 0.32231404958677684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: training accuracy: 0.46253469010175763, valid accuracy: 0.3305785123966942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.46it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: training accuracy: 0.4708603145235893, valid accuracy: 0.36363636363636365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.46it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: training accuracy: 0.47641073080481033, valid accuracy: 0.30578512396694213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.49it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: training accuracy: 0.47733580018501387, valid accuracy: 0.33884297520661155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bird_model = models.mobilenet_v2()\n",
    "num_ftrs = bird_model.classifier[1].in_features\n",
    "bird_model.classifier[1]=nn.Linear(num_ftrs, 120,bias=True)\n",
    "bird_model.load_state_dict(torch.load('dog_model.pt'))\n",
    "\n",
    "# for param in bird_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "ct = 0\n",
    "for child in bird_model.children():\n",
    "    ct += 1\n",
    "    if ct <= 20:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "num_ftrs = bird_model.classifier[1].in_features\n",
    "bird_model.classifier[1]=nn.Linear(num_ftrs, 25,bias=True)\n",
    "\n",
    "for param in bird_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bird_model = bird_model.to(device)\n",
    "\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465] \n",
    "std = [0.2470, 0.2435, 0.2616] \n",
    "batch_size = 64\n",
    "n_epochs = 100\n",
    "\n",
    "train_transform = transforms.Compose([ \n",
    "transforms.Resize((224,224)), \n",
    "transforms.RandomCrop(224, padding=4), \n",
    "transforms.RandomHorizontalFlip(),\n",
    "transforms.ToTensor(), \n",
    "transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "path='C:/Users/User/DeepLearning/Deep_Learning/Bird_Classification/HW2_Dataset/dataset_new'\n",
    "all_train = datasets.ImageFolder(root = path, transform = train_transform)\n",
    "train_size = int(0.9 * len(all_train ))\n",
    "validation_size = len(all_train) - train_size\n",
    "train_dataset, validation_dataset = random_split(all_train , [train_size, validation_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bird_model.parameters(), lr=1e-3)\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    corrects=0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad() # step 1\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "\n",
    "        logits = model(images) # step 2 (forward pass)\n",
    "        loss = loss_fn(logits, labels) # step 3 (compute loss)\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        corrects += predictions.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        loss.backward() # step 4 (backpropagation)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "       \n",
    "        \n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    \n",
    "    return train_loss, corrects/total    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, valid_loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses=0.\n",
    "    corrects=0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "                \n",
    "            logits = model(images) # step 2 (forward pass)\n",
    "            loss = loss_fn(logits, labels) # step 3 (compute loss)\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            corrects += predictions.eq(labels).sum().item()\n",
    "            \n",
    "            losses += loss.item()*images.size(0)    \n",
    "            \n",
    "        valid_loss = losses/len(valid_loader.sampler)\n",
    "    return valid_loss, corrects / total\n",
    "\n",
    "\n",
    "# is_valid_available = True\n",
    "# scheduler = lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.9 ** epoch)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0)\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "early_stopper = EarlyStopper(patience=7)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    training_loss, training_accuracy = train(bird_model, train_loader, optimizer, loss_fn)\n",
    "    valid_loss, valid_accuracy = validate(bird_model, val_loader, loss_fn)\n",
    "    \n",
    "    train_loss_list.append(training_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "\n",
    "    # if scheduler is not None and is_valid_available:\n",
    "    #     scheduler.step(valid_loss)\n",
    "    # elif scheduler is not None:\n",
    "    #     scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}: training accuracy: {training_accuracy}, valid accuracy: {valid_accuracy}\")\n",
    "    \n",
    "    if early_stopper.early_stop(valid_loss): \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.43it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: training accuracy: 0.09620721554116558, valid accuracy: 0.15702479338842976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: training accuracy: 0.24421831637372804, valid accuracy: 0.2644628099173554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.46it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: training accuracy: 0.28769657724329323, valid accuracy: 0.24793388429752067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.47it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: training accuracy: 0.32192414431082333, valid accuracy: 0.256198347107438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.46it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: training accuracy: 0.34782608695652173, valid accuracy: 0.2975206611570248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.44it/s]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: training accuracy: 0.34875115633672527, valid accuracy: 0.2975206611570248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:11<00:00,  1.45it/s]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: training accuracy: 0.3876040703052729, valid accuracy: 0.2809917355371901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [05:53<00:00, 20.82s/it]  \n",
      "100%|██████████| 2/2 [00:09<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: training accuracy: 0.3654024051803885, valid accuracy: 0.2892561983471074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:13<00:00,  1.21it/s]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: training accuracy: 0.3709528214616096, valid accuracy: 0.256198347107438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:14<00:00,  1.18it/s]\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: training accuracy: 0.3866790009250694, valid accuracy: 0.256198347107438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:14<00:00,  1.14it/s]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: training accuracy: 0.395004625346901, valid accuracy: 0.24793388429752067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:16<00:00,  1.05it/s]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: training accuracy: 0.3866790009250694, valid accuracy: 0.2892561983471074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:16<00:00,  1.06it/s]\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: training accuracy: 0.40148011100832565, valid accuracy: 0.3140495867768595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:13<00:00,  1.23it/s]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: training accuracy: 0.3940795559666975, valid accuracy: 0.2809917355371901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:15<00:00,  1.07it/s]\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: training accuracy: 0.3996299722479186, valid accuracy: 0.2809917355371901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:15<00:00,  1.12it/s]\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: training accuracy: 0.40795559666975023, valid accuracy: 0.3140495867768595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:13<00:00,  1.25it/s]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: training accuracy: 0.40425531914893614, valid accuracy: 0.30578512396694213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:15<00:00,  1.08it/s]\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: training accuracy: 0.4412580943570768, valid accuracy: 0.2809917355371901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:14<00:00,  1.19it/s]\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: training accuracy: 0.42923219241443106, valid accuracy: 0.2809917355371901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:14<00:00,  1.17it/s]\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: training accuracy: 0.42368177613321, valid accuracy: 0.2892561983471074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "bird_model = models.mobilenet_v2()\n",
    "num_ftrs = bird_model.classifier[1].in_features\n",
    "bird_model.classifier[1]=nn.Linear(num_ftrs, 120,bias=True)\n",
    "bird_model.load_state_dict(torch.load('dog_model.pt'))\n",
    "\n",
    "# for param in bird_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "ct = 0\n",
    "for child in bird_model.children():\n",
    "    ct += 1\n",
    "    if ct <= 30:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "num_ftrs = bird_model.classifier[1].in_features\n",
    "bird_model.classifier[1]=nn.Linear(num_ftrs, 25,bias=True)\n",
    "\n",
    "for param in bird_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bird_model = bird_model.to(device)\n",
    "\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465] \n",
    "std = [0.2470, 0.2435, 0.2616] \n",
    "batch_size = 64\n",
    "n_epochs = 100\n",
    "\n",
    "train_transform = transforms.Compose([ \n",
    "transforms.Resize((224,224)), \n",
    "transforms.RandomCrop(224, padding=4), \n",
    "transforms.RandomHorizontalFlip(),\n",
    "transforms.ToTensor(), \n",
    "transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "path='C:/Users/User/DeepLearning/Deep_Learning/Bird_Classification/HW2_Dataset/dataset_new'\n",
    "all_train = datasets.ImageFolder(root = path, transform = train_transform)\n",
    "train_size = int(0.9 * len(all_train ))\n",
    "validation_size = len(all_train) - train_size\n",
    "train_dataset, validation_dataset = random_split(all_train , [train_size, validation_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bird_model.parameters(), lr=1e-3)\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    corrects=0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad() # step 1\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "\n",
    "        logits = model(images) # step 2 (forward pass)\n",
    "        loss = loss_fn(logits, labels) # step 3 (compute loss)\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        corrects += predictions.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        loss.backward() # step 4 (backpropagation)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "       \n",
    "        \n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    \n",
    "    return train_loss, corrects/total    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, valid_loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses=0.\n",
    "    corrects=0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "                \n",
    "            logits = model(images) # step 2 (forward pass)\n",
    "            loss = loss_fn(logits, labels) # step 3 (compute loss)\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            corrects += predictions.eq(labels).sum().item()\n",
    "            \n",
    "            losses += loss.item()*images.size(0)    \n",
    "            \n",
    "        valid_loss = losses/len(valid_loader.sampler)\n",
    "    return valid_loss, corrects / total\n",
    "\n",
    "\n",
    "# is_valid_available = True\n",
    "# scheduler = lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.9 ** epoch)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0)\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "early_stopper = EarlyStopper(patience=7)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    training_loss, training_accuracy = train(bird_model, train_loader, optimizer, loss_fn)\n",
    "    valid_loss, valid_accuracy = validate(bird_model, val_loader, loss_fn)\n",
    "    \n",
    "    train_loss_list.append(training_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "\n",
    "    # if scheduler is not None and is_valid_available:\n",
    "    #     scheduler.step(valid_loss)\n",
    "    # elif scheduler is not None:\n",
    "    #     scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}: training accuracy: {training_accuracy}, valid accuracy: {valid_accuracy}\")\n",
    "    \n",
    "    if early_stopper.early_stop(valid_loss): \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_model = models.mobilenet_v2()\n",
    "num_ftrs = bird_model.classifier[1].in_features\n",
    "bird_model.classifier[1]=nn.Linear(num_ftrs, 120,bias=True)\n",
    "bird_model.load_state_dict(torch.load('dog_model.pt'))\n",
    "\n",
    "# for param in bird_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "ct = 0\n",
    "for child in bird_model.children():\n",
    "    ct += 1\n",
    "    if ct <= 40:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "num_ftrs = bird_model.classifier[1].in_features\n",
    "bird_model.classifier[1]=nn.Linear(num_ftrs, 25,bias=True)\n",
    "\n",
    "for param in bird_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bird_model = bird_model.to(device)\n",
    "\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465] \n",
    "std = [0.2470, 0.2435, 0.2616] \n",
    "batch_size = 64\n",
    "n_epochs = 100\n",
    "\n",
    "train_transform = transforms.Compose([ \n",
    "transforms.Resize((224,224)), \n",
    "transforms.RandomCrop(224, padding=4), \n",
    "transforms.RandomHorizontalFlip(),\n",
    "transforms.ToTensor(), \n",
    "transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "path='C:/Users/User/DeepLearning/Deep_Learning/Bird_Classification/HW2_Dataset/dataset_new'\n",
    "all_train = datasets.ImageFolder(root = path, transform = train_transform)\n",
    "train_size = int(0.9 * len(all_train ))\n",
    "validation_size = len(all_train) - train_size\n",
    "train_dataset, validation_dataset = random_split(all_train , [train_size, validation_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bird_model.parameters(), lr=1e-3)\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    corrects=0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad() # step 1\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "\n",
    "        logits = model(images) # step 2 (forward pass)\n",
    "        loss = loss_fn(logits, labels) # step 3 (compute loss)\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        corrects += predictions.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        loss.backward() # step 4 (backpropagation)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "       \n",
    "        \n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    \n",
    "    return train_loss, corrects/total    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, valid_loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses=0.\n",
    "    corrects=0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "                \n",
    "            logits = model(images) # step 2 (forward pass)\n",
    "            loss = loss_fn(logits, labels) # step 3 (compute loss)\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            corrects += predictions.eq(labels).sum().item()\n",
    "            \n",
    "            losses += loss.item()*images.size(0)    \n",
    "            \n",
    "        valid_loss = losses/len(valid_loader.sampler)\n",
    "    return valid_loss, corrects / total\n",
    "\n",
    "\n",
    "# is_valid_available = True\n",
    "# scheduler = lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.9 ** epoch)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0)\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "early_stopper = EarlyStopper(patience=7)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    training_loss, training_accuracy = train(bird_model, train_loader, optimizer, loss_fn)\n",
    "    valid_loss, valid_accuracy = validate(bird_model, val_loader, loss_fn)\n",
    "    \n",
    "    train_loss_list.append(training_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "\n",
    "    # if scheduler is not None and is_valid_available:\n",
    "    #     scheduler.step(valid_loss)\n",
    "    # elif scheduler is not None:\n",
    "    #     scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}: training accuracy: {training_accuracy}, valid accuracy: {valid_accuracy}\")\n",
    "    \n",
    "    if early_stopper.early_stop(valid_loss): \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import transforms, datasets, models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465] \n",
    "std = [0.2470, 0.2435, 0.2616] \n",
    "batch_size = 40\n",
    "n_epochs = 100\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "    # Random augmentations\n",
    "    # Randomly rotate images by 40 degrees\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),  # Random color jitter\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalize with mean and std\n",
    "])\n",
    "\n",
    "path='train'\n",
    "all_train = datasets.ImageFolder(root = path, transform = train_transform)\n",
    "test = datasets.ImageFolder(root = 'ttest', transform = train_transform)\n",
    "train_size = int(0.9 * len(all_train))\n",
    "validation_size = len(all_train) - train_size\n",
    "train_dataset, validation_dataset = random_split(all_train , [train_size, validation_size])\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (3, 224, 224)  # Example: 3 channels, 32x32 pixels\n",
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import ceil\n",
    "\n",
    "# Inverted Residual Block with Squeeze-and-Excitation\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio, stride, kernel_size, reduction_ratio=4):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.expand_ratio = expand_ratio\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        \n",
    "        # Expansion phase\n",
    "        if expand_ratio != 1:\n",
    "            self.expand_conv = nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False)\n",
    "            self.bn0 = nn.BatchNorm2d(hidden_dim)\n",
    "        else:\n",
    "            self.expand_conv = None\n",
    "        \n",
    "        # Depthwise convolution\n",
    "        self.depthwise_conv = nn.Conv2d(hidden_dim if expand_ratio != 1 else in_channels, hidden_dim, \n",
    "                                        kernel_size=kernel_size, stride=stride, \n",
    "                                        padding=kernel_size // 2, groups=hidden_dim, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "        # Squeeze and Excitation block\n",
    "        self.se_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.se_fc1 = nn.Conv2d(hidden_dim, hidden_dim // reduction_ratio, kernel_size=1)\n",
    "        self.se_fc2 = nn.Conv2d(hidden_dim // reduction_ratio, hidden_dim, kernel_size=1)\n",
    "        \n",
    "        # Output phase\n",
    "        self.project_conv = nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.use_residual = (in_channels == out_channels and stride == 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        if self.expand_conv:\n",
    "            out = F.relu6(self.bn0(self.expand_conv(x)))\n",
    "        else:\n",
    "            out = x\n",
    "        \n",
    "        # Depthwise convolution\n",
    "        out = F.relu6(self.bn1(self.depthwise_conv(out)))\n",
    "        \n",
    "        # Squeeze and Excitation\n",
    "        se = self.se_avgpool(out)\n",
    "        se = F.relu(self.se_fc1(se))\n",
    "        se = torch.sigmoid(self.se_fc2(se))\n",
    "        out = out * se\n",
    "        \n",
    "        # Output\n",
    "        out = self.bn2(self.project_conv(out))\n",
    "        \n",
    "        if self.use_residual:\n",
    "            out = out + identity\n",
    "        \n",
    "        return out\n",
    "\n",
    "# EfficientNet Main Architecture\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, width_coefficient=0.3, depth_coefficient=0.7, dropout_rate=0.2, num_classes=100):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        \n",
    "        # Base settings for EfficientNet with reduced coefficients\n",
    "        base_channels = 16  # Further reduced base channels\n",
    "        base_layers = [\n",
    "            # (expand_ratio, out_channels, num_blocks, stride, kernel_size)\n",
    "            (1, 16, 1, 1, 3),   # Stage 1\n",
    "            (6, 24, 1, 2, 3),   # Stage 2\n",
    "            (6, 40, 1, 2, 5),   # Stage 3\n",
    "            (6, 80, 2, 2, 3),   # Stage 4\n",
    "            (6, 112, 2, 1, 5),  # Stage 5\n",
    "            (6, 192, 2, 2, 5),  # Stage 6\n",
    "            (6, 320, 1, 1, 3)   # Stage 7\n",
    "        ]\n",
    "        \n",
    "        # Stem\n",
    "        out_channels = ceil(base_channels * width_coefficient)\n",
    "        self.stem_conv = nn.Conv2d(3, out_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.stem_bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Build blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        in_channels = out_channels\n",
    "        for expand_ratio, out_channels, num_blocks, stride, kernel_size in base_layers:\n",
    "            out_channels = ceil(out_channels * width_coefficient)\n",
    "            num_blocks = ceil(num_blocks * depth_coefficient)\n",
    "            for i in range(num_blocks):\n",
    "                block_stride = stride if i == 0 else 1\n",
    "                self.blocks.append(MBConvBlock(in_channels, out_channels, expand_ratio, block_stride, kernel_size))\n",
    "                in_channels = out_channels\n",
    "        \n",
    "        # Head\n",
    "        final_channels = ceil(640 * width_coefficient)  # Further reduced head channels\n",
    "        self.head_conv = nn.Conv2d(in_channels, final_channels, kernel_size=1, bias=False)\n",
    "        self.head_bn = nn.BatchNorm2d(final_channels)\n",
    "        \n",
    "        # Pooling and classification\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(final_channels, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        x = F.relu6(self.stem_bn(self.stem_conv(x)))\n",
    "        \n",
    "        # Blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Head\n",
    "        x = F.relu6(self.head_bn(self.head_conv(x)))\n",
    "        \n",
    "        # Pooling and classification\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parameters: 409630\n"
     ]
    }
   ],
   "source": [
    "model =EfficientNet().to(device)\n",
    "# prune.l1_unstructured(model.fc, name='weight', amount=0.4)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"# parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:58<00:00,  5.40it/s]\n",
      "100%|██████████| 35/35 [00:12<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "Train Loss: 4.2428, Train Acc: 0.0429\n",
      "Val Loss: 3.9635, Val Acc: 0.0650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:30<00:00, 10.22it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:\n",
      "Train Loss: 3.7173, Train Acc: 0.0986\n",
      "Val Loss: 3.5698, Val Acc: 0.1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.31it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:\n",
      "Train Loss: 3.3969, Train Acc: 0.1537\n",
      "Val Loss: 3.3557, Val Acc: 0.1593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.43it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100:\n",
      "Train Loss: 3.1926, Train Acc: 0.1877\n",
      "Val Loss: 3.1367, Val Acc: 0.1921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.40it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100:\n",
      "Train Loss: 3.0201, Train Acc: 0.2181\n",
      "Val Loss: 2.9177, Val Acc: 0.2550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.18it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100:\n",
      "Train Loss: 2.8653, Train Acc: 0.2494\n",
      "Val Loss: 2.9294, Val Acc: 0.2421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:36<00:00,  8.71it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100:\n",
      "Train Loss: 2.7439, Train Acc: 0.2822\n",
      "Val Loss: 2.6737, Val Acc: 0.3086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.08it/s]\n",
      "100%|██████████| 35/35 [00:12<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100:\n",
      "Train Loss: 2.6256, Train Acc: 0.3083\n",
      "Val Loss: 2.6223, Val Acc: 0.3271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:57<00:00,  5.46it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100:\n",
      "Train Loss: 2.5246, Train Acc: 0.3315\n",
      "Val Loss: 2.5262, Val Acc: 0.3343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.48it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100:\n",
      "Train Loss: 2.4209, Train Acc: 0.3551\n",
      "Val Loss: 2.4205, Val Acc: 0.3479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:35<00:00,  8.97it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100:\n",
      "Train Loss: 2.3199, Train Acc: 0.3752\n",
      "Val Loss: 2.3555, Val Acc: 0.3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.36it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100:\n",
      "Train Loss: 2.2516, Train Acc: 0.3927\n",
      "Val Loss: 2.2968, Val Acc: 0.3807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.20it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100:\n",
      "Train Loss: 2.1654, Train Acc: 0.4085\n",
      "Val Loss: 2.2247, Val Acc: 0.4250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.30it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100:\n",
      "Train Loss: 2.0918, Train Acc: 0.4307\n",
      "Val Loss: 2.2068, Val Acc: 0.4036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.23it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100:\n",
      "Train Loss: 2.0408, Train Acc: 0.4466\n",
      "Val Loss: 2.0976, Val Acc: 0.4379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:33<00:00,  9.28it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100:\n",
      "Train Loss: 1.9809, Train Acc: 0.4556\n",
      "Val Loss: 2.0499, Val Acc: 0.4514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.21it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100:\n",
      "Train Loss: 1.9077, Train Acc: 0.4700\n",
      "Val Loss: 1.9911, Val Acc: 0.4693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.19it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100:\n",
      "Train Loss: 1.8401, Train Acc: 0.4898\n",
      "Val Loss: 1.9727, Val Acc: 0.4714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.18it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100:\n",
      "Train Loss: 1.7969, Train Acc: 0.5019\n",
      "Val Loss: 2.0100, Val Acc: 0.4786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.15it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100:\n",
      "Train Loss: 1.7413, Train Acc: 0.5135\n",
      "Val Loss: 1.9629, Val Acc: 0.4579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.12it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100:\n",
      "Train Loss: 1.7070, Train Acc: 0.5264\n",
      "Val Loss: 1.9043, Val Acc: 0.4757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:35<00:00,  8.93it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100:\n",
      "Train Loss: 1.6574, Train Acc: 0.5331\n",
      "Val Loss: 1.8752, Val Acc: 0.4843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.05it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100:\n",
      "Train Loss: 1.5946, Train Acc: 0.5530\n",
      "Val Loss: 1.8251, Val Acc: 0.5007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.12it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100:\n",
      "Train Loss: 1.5742, Train Acc: 0.5536\n",
      "Val Loss: 1.7680, Val Acc: 0.5207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:35<00:00,  8.93it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100:\n",
      "Train Loss: 1.5347, Train Acc: 0.5705\n",
      "Val Loss: 1.7077, Val Acc: 0.5321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:35<00:00,  8.98it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100:\n",
      "Train Loss: 1.4889, Train Acc: 0.5767\n",
      "Val Loss: 1.7376, Val Acc: 0.5193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.02it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100:\n",
      "Train Loss: 1.4421, Train Acc: 0.5909\n",
      "Val Loss: 1.7068, Val Acc: 0.5336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:35<00:00,  8.95it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100:\n",
      "Train Loss: 1.4157, Train Acc: 0.5955\n",
      "Val Loss: 1.7211, Val Acc: 0.5279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:35<00:00,  8.97it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100:\n",
      "Train Loss: 1.3829, Train Acc: 0.6087\n",
      "Val Loss: 1.7001, Val Acc: 0.5321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [01:00<00:00,  5.20it/s]\n",
      "100%|██████████| 35/35 [00:13<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100:\n",
      "Train Loss: 1.3260, Train Acc: 0.6206\n",
      "Val Loss: 1.7141, Val Acc: 0.5429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [01:08<00:00,  4.58it/s]\n",
      "100%|██████████| 35/35 [00:11<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100:\n",
      "Train Loss: 1.3296, Train Acc: 0.6241\n",
      "Val Loss: 1.6366, Val Acc: 0.5621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [01:08<00:00,  4.60it/s]\n",
      "100%|██████████| 35/35 [00:11<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100:\n",
      "Train Loss: 1.2778, Train Acc: 0.6324\n",
      "Val Loss: 1.6346, Val Acc: 0.5557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [01:08<00:00,  4.62it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100:\n",
      "Train Loss: 1.2521, Train Acc: 0.6407\n",
      "Val Loss: 1.5960, Val Acc: 0.5621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [01:08<00:00,  4.61it/s]\n",
      "100%|██████████| 35/35 [00:12<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100:\n",
      "Train Loss: 1.2166, Train Acc: 0.6495\n",
      "Val Loss: 1.5930, Val Acc: 0.5629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [01:07<00:00,  4.63it/s]\n",
      "100%|██████████| 35/35 [00:11<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100:\n",
      "Train Loss: 1.1832, Train Acc: 0.6592\n",
      "Val Loss: 1.5660, Val Acc: 0.5786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [01:06<00:00,  4.71it/s]\n",
      "100%|██████████| 35/35 [00:13<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100:\n",
      "Train Loss: 1.1606, Train Acc: 0.6625\n",
      "Val Loss: 1.5648, Val Acc: 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [01:07<00:00,  4.66it/s]\n",
      "100%|██████████| 35/35 [00:12<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100:\n",
      "Train Loss: 1.1433, Train Acc: 0.6690\n",
      "Val Loss: 1.5658, Val Acc: 0.5821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [01:09<00:00,  4.52it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100:\n",
      "Train Loss: 1.1022, Train Acc: 0.6862\n",
      "Val Loss: 1.5169, Val Acc: 0.5871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:39<00:00,  8.07it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100:\n",
      "Train Loss: 1.0876, Train Acc: 0.6863\n",
      "Val Loss: 1.4883, Val Acc: 0.5979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:32<00:00,  9.55it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100:\n",
      "Train Loss: 1.0638, Train Acc: 0.6910\n",
      "Val Loss: 1.4862, Val Acc: 0.6029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:55<00:00,  5.65it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100:\n",
      "Train Loss: 1.0165, Train Acc: 0.6990\n",
      "Val Loss: 1.5757, Val Acc: 0.5779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:37<00:00,  8.47it/s]\n",
      "100%|██████████| 35/35 [00:12<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100:\n",
      "Train Loss: 1.0006, Train Acc: 0.7057\n",
      "Val Loss: 1.4730, Val Acc: 0.6157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:37<00:00,  8.32it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100:\n",
      "Train Loss: 0.9612, Train Acc: 0.7184\n",
      "Val Loss: 1.4679, Val Acc: 0.5936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:41<00:00,  7.51it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100:\n",
      "Train Loss: 0.9575, Train Acc: 0.7164\n",
      "Val Loss: 1.5557, Val Acc: 0.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:36<00:00,  8.58it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100:\n",
      "Train Loss: 0.9217, Train Acc: 0.7250\n",
      "Val Loss: 1.5373, Val Acc: 0.5943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:35<00:00,  8.91it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100:\n",
      "Train Loss: 0.9104, Train Acc: 0.7301\n",
      "Val Loss: 1.3858, Val Acc: 0.6171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.07it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100:\n",
      "Train Loss: 0.8983, Train Acc: 0.7308\n",
      "Val Loss: 1.4178, Val Acc: 0.6143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:34<00:00,  9.07it/s]\n",
      "100%|██████████| 35/35 [00:11<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100:\n",
      "Train Loss: 0.8544, Train Acc: 0.7445\n",
      "Val Loss: 1.4470, Val Acc: 0.6050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:39<00:00,  8.00it/s]\n",
      "100%|██████████| 35/35 [00:11<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100:\n",
      "Train Loss: 0.8583, Train Acc: 0.7453\n",
      "Val Loss: 1.5493, Val Acc: 0.5964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:36<00:00,  8.73it/s]\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100:\n",
      "Train Loss: 0.8351, Train Acc: 0.7460\n",
      "Val Loss: 1.4203, Val Acc: 0.6214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:35<00:00,  8.98it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100:\n",
      "Train Loss: 0.8053, Train Acc: 0.7614\n",
      "Val Loss: 1.4482, Val Acc: 0.6157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:40<00:00,  7.77it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100:\n",
      "Train Loss: 0.7849, Train Acc: 0.7632\n",
      "Val Loss: 1.5442, Val Acc: 0.5943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:36<00:00,  8.59it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100:\n",
      "Train Loss: 0.7789, Train Acc: 0.7644\n",
      "Val Loss: 1.4165, Val Acc: 0.6314\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=False)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=1e-6)\n",
    "\n",
    "# Early stopping class\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Train function\n",
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "# Validation function\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, loss_fn):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(val_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        val_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    avg_loss = val_loss / len(val_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training loop\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "early_stopper = EarlyStopper(patience=7)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, loss_fn)\n",
    "    val_loss, val_acc = validate(model, val_loader, loss_fn)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopper.early_stop(val_loss):\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), \"final_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Top-5 Accuracy: 97.00%\n",
      "Predictions saved to result.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Custom Dataset for Test Images\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# Import the custom EfficientNet model\n",
    "# prune.remove(model.fc, 'weight')\n",
    "\n",
    "# # Then save the model state\n",
    "# torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "def test():\n",
    "    # Load the trained model\n",
    "    model = EfficientNet()  # Initialize the model with 100 classes\n",
    "    model.load_state_dict(torch.load('best_model.pth'))  # Assuming you saved the trained model as model.pth\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # # Data transformations\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Resize((224, 224)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    # ])\n",
    "\n",
    "    # Load the test dataset\n",
    "    # test_dataset = datasets.ImageFolder(root='ttest', transform=transform)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    total, correct_top5 = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            # _, predicted = torch.max(outputs, 1)\n",
    "            _, predicted_top5 = torch.topk(outputs, 5, dim=1)\n",
    "            for i in range(labels.size(0)):\n",
    "                if labels[i] in predicted_top5[i]:\n",
    "                    correct_top5 += 1\n",
    "            total += labels.size(0)\n",
    "            # correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # accuracy = correct / total\n",
    "    # print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "    top5_accuracy = correct_top5 / total\n",
    "    print(f'Test Top-5 Accuracy: {top5_accuracy * 100:.2f}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_dir, transform=None):\n",
    "        self.test_dir = test_dir\n",
    "        self.file_names = os.listdir(test_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.file_names[idx]\n",
    "        img_path = os.path.join(self.test_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name\n",
    "\n",
    "# Load Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EfficientNet().to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Define transformations for train and test datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "    # Random augmentations\n",
    "    # Randomly rotate images by 40 degrees\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),  # Random color jitter\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalize with mean and std\n",
    "])\n",
    "\n",
    "# Load the train dataset using ImageFolder\n",
    "train_dir = os.path.abspath('./train')  # Replace with the path to your train dataset\n",
    "train_dataset = ImageFolder(train_dir, transform=transform)\n",
    "\n",
    "# Get the class_to_idx from the training dataset and invert it to create idx_to_class\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}  # Invert class_to_idx to idx_to_class\n",
    "\n",
    "# Load test dataset\n",
    "test_dir = os.path.abspath('./test')  # Path to your test dataset\n",
    "test_dataset = TestDataset(test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Prepare CSV for results\n",
    "results = []\n",
    "\n",
    "# Make predictions and store them in result.csv\n",
    "with torch.no_grad():\n",
    "    for images, file_names in test_loader:  # file_names returned from the dataset\n",
    "        images = images.to(device)  # Move images to GPU/CPU\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Get top-5 predictions for each image\n",
    "        _, top5_preds = torch.topk(outputs, 5, dim=1)\n",
    "        \n",
    "        for i, preds in enumerate(top5_preds):\n",
    "            file_name = file_names[i]  # Get the file name\n",
    "            # Convert predicted indices to class labels (words)\n",
    "            pred_labels = [idx_to_class[pred.item()] for pred in preds]\n",
    "            \n",
    "            # Append to results\n",
    "            results.append([file_name] + pred_labels)\n",
    "\n",
    "# Save results to CSV\n",
    "df = pd.DataFrame(results, columns=['file_name', 'pred1', 'pred2', 'pred3', 'pred4', 'pred5'])\n",
    "df.to_csv('result.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to result.csv\")\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# def test():\n",
    "#     # Load the trained model\n",
    "#     model = efficientnet_custom(num_classes=100) \n",
    "#     model.load_state_dict(torch.load('best_model.pth'))  # Load the trained model\n",
    "#     model.eval()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     # Data transformations\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     # Load the test dataset\n",
    "#     test_dataset = datasets.ImageFolder(root='ttest', transform=transform)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "#      # Mapping from index to class label (word)\n",
    "#     idx_to_class = {v: k for k, v in test_dataset.class_to_idx.items()}\n",
    "\n",
    "#     # Create CSV file with predictions\n",
    "#     with open('result.csv', 'w') as f:\n",
    "#         # Write header\n",
    "#         f.write('file_name,pred1,pred2,pred3,pred4,pred5\\n')\n",
    "\n",
    "#         # Make predictions and store them in result.csv\n",
    "#         with torch.no_grad():\n",
    "#             for i, (images, labels) in enumerate(test_loader):\n",
    "#                 outputs = model(images)\n",
    "#                 _, top5_preds = torch.topk(outputs, 5, dim=1)\n",
    "\n",
    "#                 for j, preds in enumerate(top5_preds):\n",
    "#                     # Get the file name for the current image\n",
    "#                     file_name = test_dataset.samples[i * test_loader.batch_size + j][0]\n",
    "\n",
    "#                     # Convert predicted indices to class labels (words)\n",
    "#                     pred_labels = [idx_to_class[pred.item()] for pred in preds]\n",
    "\n",
    "#                     # Write to CSV\n",
    "#                     f.write(f'{file_name},{pred_labels[0]},{pred_labels[1]},{pred_labels[2]},{pred_labels[3]},{pred_labels[4]}\\n')\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test()\n",
    "\n",
    "#     results = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images = images.to(device)\n",
    "#             outputs = model(images)\n",
    "#             # Get the top 5 predictions\n",
    "#             _, top5 = torch.topk(outputs, 5, dim=1)\n",
    "\n",
    "#             # Loop through the batch and save results\n",
    "#             for i, idx in enumerate(top5):\n",
    "#                 file_name = test_dataset.samples[i][0]  # Get the image file path\n",
    "#                 file_name = os.path.basename(file_name)  # Get the file name only\n",
    "#                 pred_labels = idx.cpu().numpy().tolist()  # Convert to list of top 5 predictions\n",
    "#                 results.append([file_name] + pred_labels)\n",
    "\n",
    "#     # Write the results to a CSV file\n",
    "#     with open('result.csv', mode='w', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(['file_name', 'pred1', 'pred2', 'pred3', 'pred4', 'pred5'])  # Write header\n",
    "#         writer.writerows(results)  # Write all rows of results\n",
    "\n",
    "#     print(f'Results saved to result.csv')\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

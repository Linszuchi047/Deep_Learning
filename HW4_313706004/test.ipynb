{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from timm.models.vision_transformer import Attention\n",
    "import cv2\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "class GradientRollout:\n",
    "    def __init__(self):\n",
    "        self.model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.attentions = []\n",
    "        self.attention_grads = []\n",
    "        self.num_heads = 3\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406),  # ImageNet mean\n",
    "                std=(0.229, 0.224, 0.225)    # ImageNet std\n",
    "            )\n",
    "        ])\n",
    "        # Replace the 'forward' method of all 'Attention' modules \n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, Attention):\n",
    "                # Bind the attention_forward method to the module instance\n",
    "                module.forward = self.attention_forward.__get__(module, Attention)\n",
    "\n",
    "                # Register the forward hook to extract attention weights\n",
    "                module.register_forward_hook(self.get_attention)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention_forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        TODO:\n",
    "            Implement the attention computation and store the attention maps.\n",
    "            You need to save the attention map into variable \"self.attn_weights\"\n",
    "\n",
    "            Note: Due to @staticmethod, \"self\" here refers to the \"Attention\" module instance, not the class itself.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # write your code here\n",
    "        self.attn_weights = None # save the attention map into this variable\n",
    "        pass\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.scale)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        self.attn_weights = attn  # Save the attention map\n",
    "        return (attn @ v).transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
    "\n",
    "    # Define a hook function to extract attention weights\n",
    "    def get_attention(self, module, input, output):\n",
    "        # Append the attention weights\n",
    "        self.attentions.append(module.attn_weights.detach().cpu())\n",
    "\n",
    "        # Retain gradients on attention weights\n",
    "        module.attn_weights.retain_grad()\n",
    "\n",
    "        # Register a hook on attn_weights to save the gradients during backward pass\n",
    "        def save_attn_grad(grad):\n",
    "            self.attention_grads.append(grad.cpu())\n",
    "        module.attn_weights.register_hook(save_attn_grad)\n",
    "\n",
    "    def clear_attentions(self):\n",
    "        # clear the stored attention weights\n",
    "        self.attentions = []\n",
    "        self.attention_grads = []\n",
    "    \n",
    "    \n",
    "    def gradient_rollout(self, discard_ratio = 0.9):\n",
    "        \"\"\"\n",
    "\n",
    "        TODO:\n",
    "            Define the attention rollout function that accumulate the final attention flows.\n",
    "            You need to return parameter \"mask\", which should be the final attention flows.  \n",
    "            You can follow the below procedures:\n",
    "\n",
    "            For each attention layers:\n",
    "                1. perform matrix mutiplication on current attention map and gradient\n",
    "                2. filter the attention by mean/min/max fiter with respect to 3 attention heads\n",
    "                3. wipe out all negative values to 0\n",
    "                4. (optional) normalize the attention map of current layer\n",
    "                5. perform matrix multiplication on current attention map and previous results\n",
    "\n",
    "            (6. and 7. are already done for you, you just need to get the variable \"result\" correctly.)\n",
    "            6. Obtain the mask: Gather the attention flows that use [CLS] token as query and the rest tokens as keys\n",
    "            7. Normalize the values inside the mask to [0.0, 1.0]\n",
    "\n",
    "        \"\"\"\n",
    "        result = torch.eye(self.attentions[0].size(-1))\n",
    "        \n",
    "        for attention, grad in zip(self.attentions, self.attention_grads): \n",
    "            # Write your code(Gradient Rollout) here to update \"result\" matrix\n",
    "            # 使用梯度作为注意力的权重\n",
    "                weights = grad\n",
    "                attention_heads_fused = (attention * weights).mean(dim=1)\n",
    "\n",
    "                # 将小于0的注意力值设为0（因为负值可能没有意义）\n",
    "                attention_heads_fused[attention_heads_fused < 0] = 0\n",
    "\n",
    "                # 丢弃最小的注意力值，但保留 [CLS] token\n",
    "                flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n",
    "                _, indices = flat.topk(int(flat.size(-1) * discard_ratio), dim=-1, largest=False)\n",
    "                flat[0, indices] = 0\n",
    "\n",
    "                # 添加单位矩阵用于保持原始的自注意力特性\n",
    "                I = torch.eye(attention_heads_fused.size(-1), device=attention_heads_fused.device)\n",
    "                a = (attention_heads_fused + I) / 2\n",
    "                a = a / a.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                # 逐层累积注意力\n",
    "                result = torch.matmul(a, result)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        if you have variable \"result\" in correct shape(shape of attenion map), then this part should work properly.\n",
    "        \"\"\"\n",
    "        # Look at the total attention between the class token,\n",
    "        # and the image patches\n",
    "        mask = result[0, 0 , 1 :]\n",
    "        # In case of 224x224 image, this brings us from 196 to 14\n",
    "        width = int(mask.size(-1)**0.5)\n",
    "        mask = mask.reshape(width, width).numpy()\n",
    "        mask = mask / np.max(mask)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def perform_backpropagation(self, input_tensor, target_category):\n",
    "        \"\"\"\n",
    "\n",
    "        TODO:\n",
    "            In order to get gradients, you need to do backpropagation mannually.\n",
    "            You can follow the below procedures:\n",
    "\n",
    "            1.Inference the model\n",
    "            2.define a loss simple function that focus on the target category\n",
    "            2-1. The loss function can be sum of the logits of target category or\n",
    "            2-2. cross entropy of target category\n",
    "            3.perform backpropagation with respect to the loss.\n",
    "\n",
    "            This function will be invoked in \"run\" function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # write your code here to perform backpropagation\n",
    "        pass\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # 前向传播并计算输出\n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "         # 创建类别掩码，确保仅对目标类别进行反向传播\n",
    "        category_mask = torch.zeros_like(output)\n",
    "        category_mask[:, target_category] = 1\n",
    "        \n",
    "        # 计算损失，保留目标类别的损失\n",
    "        loss = (output * category_mask).sum()\n",
    "    \n",
    "\n",
    "        # 反向傳播計算梯度\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "    def show_mask_on_image(self, img, mask):\n",
    "\n",
    "        \"\"\"Do not modify this part\"\"\"\n",
    "\n",
    "        # Normalize the value of img to [0.0, 1.0]\n",
    "        img = np.float32(img) / 255\n",
    "\n",
    "        # Reshape the mask to 224x224 for later computation\n",
    "        mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
    "\n",
    "        # Generate heatmap and normalize the value\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "        # Add heatmap and original image together and normalize the value\n",
    "        combination = heatmap + np.float32(img)\n",
    "        combination = combination / np.max(combination)\n",
    "        \n",
    "        # Scale back the value to [0.0, 255.0]\n",
    "        combination = np.uint8(255 * combination)\n",
    "\n",
    "        return combination\n",
    "    \n",
    "    def run(self, image_path, target_category):\n",
    "\n",
    "        \"\"\"Do not modify this part\"\"\"\n",
    "\n",
    "        # clean previous attention maps and result\n",
    "        self.clear_attentions()\n",
    "        \n",
    "        # get the image name for saving output image2\n",
    "        image_name = os.path.basename(image_path)  # e.g., 'image.jpg'\n",
    "        image_name, _ = os.path.splitext(image_name)  # ('image', '.jpg')\n",
    "        \n",
    "        # convert image to a tensor\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = self.transform(img).unsqueeze(0)  # Add batch dimension\n",
    "        input_tensor.requires_grad_(True)\n",
    "\n",
    "        # do backpropagation manually during inference to produce gradients\n",
    "        self.perform_backpropagation(input_tensor, target_category)\n",
    "\n",
    "        np_img = np.array(img)[:, :, ::-1]\n",
    "        mask = self.gradient_rollout()\n",
    "        output_heatmap = self.show_mask_on_image(np_img, mask)\n",
    "        output_filename = f\"gradient_result_{image_name}.png\"  \n",
    "        cv2.imwrite(output_filename, output_heatmap)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     \"\"\"Do not modify this part\"\"\"\n",
    "\n",
    "#     # arg parsing\n",
    "#     parser = argparse.ArgumentParser(description='Process an image for attention visualization with respect to target category.')\n",
    "#     parser.add_argument('--image', type=str, required=True, help='Path to the input image.')\n",
    "#     parser.add_argument('--category', type=int, required=True, help=\"target category of attention flows\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # Execution\n",
    "#     model = GradientRollout()\n",
    "#     outputs = model.run(args.image, args.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "c:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "c:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_deit_main\\models.py:62: UserWarning: Overwriting deit_tiny_patch16_224 in registry with models.deit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_deit_main\\models.py:77: UserWarning: Overwriting deit_small_patch16_224 in registry with models.deit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_deit_main\\models.py:92: UserWarning: Overwriting deit_base_patch16_224 in registry with models.deit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_deit_main\\models.py:107: UserWarning: Overwriting deit_tiny_distilled_patch16_224 in registry with models.deit_tiny_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_deit_main\\models.py:122: UserWarning: Overwriting deit_small_distilled_patch16_224 in registry with models.deit_small_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_deit_main\\models.py:137: UserWarning: Overwriting deit_base_distilled_patch16_224 in registry with models.deit_base_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_deit_main\\models.py:152: UserWarning: Overwriting deit_base_patch16_384 in registry with models.deit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "C:\\Users\\User/.cache\\torch\\hub\\facebookresearch_deit_main\\models.py:167: UserWarning: Overwriting deit_base_distilled_patch16_384 in registry with models.deit_base_distilled_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "can't retain_grad on Tensor that has requires_grad=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 確認輸出和損失\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 21\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 前向傳播輸出\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     target_logits \u001b[38;5;241m=\u001b[39m output[:, target_category]\n\u001b[0;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m target_logits\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\timm\\models\\vision_transformer.py:829\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 829\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    830\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\timm\\models\\vision_transformer.py:810\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    808\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\timm\\models\\vision_transformer.py:165\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 165\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    166\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1581\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1581\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1584\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[1;32mIn[2], line 66\u001b[0m, in \u001b[0;36mGradientRollout.get_attention\u001b[1;34m(self, module, input, output)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattentions\u001b[38;5;241m.\u001b[39mappend(module\u001b[38;5;241m.\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Retain gradients on attention weights\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretain_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Register a hook on attn_weights to save the gradients during backward pass\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_attn_grad\u001b[39m(grad):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: can't retain_grad on Tensor that has requires_grad=False"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# 初始化 GradientRollout 類\n",
    "model = GradientRollout()\n",
    "\n",
    "# 測試圖片和目標類別\n",
    "image_path = \"./images/part2/87_dogbird.png\"\n",
    "target_category = 161\n",
    "\n",
    "# 加載圖片，轉換為張量\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "input_tensor = model.transform(img).unsqueeze(0)  # 添加 batch 維度\n",
    "input_tensor.requires_grad_(True)\n",
    "\n",
    "# 執行 perform_backpropagation\n",
    "model.perform_backpropagation(input_tensor, target_category)\n",
    "\n",
    "# 確認輸出和損失\n",
    "with torch.no_grad():\n",
    "    output = model.model(input_tensor)  # 前向傳播輸出\n",
    "    target_logits = output[:, target_category]\n",
    "    loss = target_logits.sum()\n",
    "\n",
    "# 打印輸出和損失\n",
    "print(f\"Model output shape: {output.shape}\")\n",
    "print(f\"Target logits for category {target_category}: {target_logits.item()}\")\n",
    "print(f\"Loss value: {loss.item()}\")\n",
    "\n",
    "# 檢查梯度\n",
    "print(f\"Gradients for input tensor (sum): {input_tensor.grad.sum()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
